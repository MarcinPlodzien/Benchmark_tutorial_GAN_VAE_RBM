# Classical Generative Benchmarks

A rigorous, meticulously implemented benchmarking suite for three fundamental families of classical generative networks: **Generative Adversarial Networks (GAN)**, **Variational Autoencoders (VAE)**, and **Restricted Boltzmann Machines (RBM)**.

This project is engineered to facilitate strict, "apples-to-apples" comparisons between diverse architectures. By enforcing a **Universal Parameter Budget**, we control for model capacity, isolating the algorithmic efficiency of each generative approach. The implementation utilizes **JAX** for high-performance, JIT-compiled execution.

---

## 1. Generative Adversarial Network (GAN)

### 1.1 Theoretical Framework
The GAN treats generative modeling as a zero-sum minimax game between two neural networks:
1.  **Generator ($G$)**: Attempts to map a latent noise vector $z \sim \mathcal{N}(0, I)$ to a sample $x_{fake}$ that is indistinguishable from real data.
2.  **Discriminator ($D$)**: Attempts to distinguish between real data samples $x_{real}$ and generated samples $x_{fake}$.

The Nash Equilibrium of this game occurs when the Generator recovers the true data distribution $p_{data}$, and the Discriminator outputs $0.5$ everywhere (random guessing).

### 1.2 Architecture
*   **Generator**: A Multi-Layer Perceptron (MLP) mapping 16-dimensional latent noise to the data dimension $D_{out}$.
    *   *Input*: $z \in \mathbb{R}^{16}$
    *   *Hidden*: Dense Layer with `LeakyReLU` activation.
    *   *Output*: Linear Layer (Data Space).
*   **Discriminator**: An MLP mapping data to a scalar probability logit.
    *   *Input*: $x \in \mathbb{R}^{D_{in}}$
    *   *Hidden*: Dense Layer with `LeakyReLU` activation.
    *   *Output*: Scalar Linear (Logit).

### 1.3 Loss Function & Training Logic
The standard Minimax loss ($\min \log(1-D(G(z)))$) suffers from vanishing gradients early in training. We implement the **Non-Saturating Heuristic Loss**:

**1. Discriminator Step**:
Maximize the log-likelihood of correctly classifying real and fake data.
$$ \mathcal{L}_D = - \left( \mathbb{E}_{x \sim data}[\log D(x)] + \mathbb{E}_{z \sim noise}[\log(1 - D(G(z)))] \right) $$
*   **Implementation Detail**: We use **Label Smoothing** for the real class. Instead of targeting $1.0$, the Discriminator is trained to predict $0.9$. This regularizes the Discriminator, preventing it from becoming too confident and providing "flat" gradients to the Generator.

**2. Generator Step**:
Maximize the probability of the Discriminator being mistaken (classifying fake data as real).
$$ \mathcal{L}_G = - \mathbb{E}_{z \sim noise}[\log D(G(z))] $$

---

## 2. Variational Autoencoder (VAE)

### 2.1 Theoretical Framework
The VAE represents a Probabilistic Graphical Model. It assumes data is generated by unseen latent variables $z$. Since the true posterior $p(z|x)$ is intractable, the VAE uses **Variational Inference** to approximate it with a parametric distribution $q_\phi(z|x)$ (the Inference Network or Encoder).

### 2.2 Architecture
*   **Encoder (Inference Net)**: Maps input $x$ to the mean $\mu$ and log-variance $\log\sigma^2$ of the latent Gaussian.
    *   *Input*: $x \in \mathbb{R}^{D_{in}}$
    *   *Output*: $2 \times Z_{dim}$ (Mean vector + LogVar vector).
*   **Decoder (Generative Net)**: Maps sampled latent $z$ back to data space.
    *   *Input*: $z \in \mathbb{R}^{Z_{dim}}$
    *   *Output*: Reconstruction $\hat{x} \in \mathbb{R}^{D_{out}}$.

### 2.3 The Reparameterization Trick
To allow gradient descent to propagate through the stochastic sampling step $z \sim \mathcal{N}(\mu, \sigma^2)$, we reparameterize the randomness:
$$ z = \mu + \sigma \odot \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) $$
Now, the stochasticity is injected via $\epsilon$ (a constant input node), allowing $\mu$ and $\sigma$ to be differentiable parameters.

### 2.4 Loss Function (ELBO)
We minimize the Negative Evidence Lower Bound (ELBO):
$$ \mathcal{L}_{VAE} = \underbrace{ ||x - \hat{x}||^2 }_{\text{Reconstruction (MSE)}} + \beta \cdot \underbrace{ D_{KL}( \mathcal{N}(\mu, \sigma) || \mathcal{N}(0, I) ) }_{\text{Regularization}} $$

*   **Reconstruction**: Measures how well the VAE preserves information.
*   **KL Divergence**: Forces the latent space to approximate a standard Normal distribution, preventing overfitting and ensuring the latent space is continuous (smooth interpolation).
*   **$\beta$-VAE**: We use $\beta=0.1$ to weigh the reconstruction error higher than the KL term, which results in sharper samples at the cost of a slightly less regularized latent manifold.

---

## 3. Restricted Boltzmann Machine (RBM)

### 3.1 Theoretical Framework
The RBM is an **Energy-Based Model (EBM)**. It defines a scalar "Energy" $E(v, h)$ for every configuration of visible units $v$ (data) and hidden units $h$ (features). The probability of a state is given by the Boltzmann distribution:
$$ P(v) = \frac{\sum_h e^{-E(v,h)}}{Z} $$
where $Z$ is the partition function (normalization constant). Learning consists of shaping the energy landscape such that real data points have low energy (high probability) and noise has high energy.

### 3.2 Architecture
The RBM is a bipartite graph (no intra-layer connections):
*   **Visible Layer ($v$)**: Corresponds to the input data dimensions.
*   **Hidden Layer ($h$)**: Corresponds to latent feature detectors.
*   **Weights ($W$)**: A learnable matrix connecting every $v_i$ to every $h_j$.

### 3.3 Training Logic: Contrastive Divergence (CD-k)
Exact Maximum Likelihood training is intractable because computing $Z$ requires summing over all possible states. We approximate the gradient using **Contrastive Divergence**:

$$ \nabla \mathcal{L} \approx \underbrace{\nabla \text{FreeEnergy}(v_{data})}_{\text{Positive Phase}} - \underbrace{\nabla \text{FreeEnergy}(v_{model})}_{\text{Negative Phase}} $$

1.  **Positive Phase**: We clamp the visible units to the real data $v_{data}$ and compute the Free Energy. This pushes the energy of real data *down*.
2.  **Negative Phase**: We estimate the model's current distribution by running a **Gibbs Sampling** chain starting from the data.
    *   $P(h|v) = \sigma(vW + b_h)$
    *   $P(v|h) \sim \mathcal{N}(hW^T + b_v, \sigma^2)$
    *   After $k$ steps, we obtain $v_{model}$. We push the energy of this "fantasy particle" *up*.
    
This "push-down, push-up" mechanism creates an energy valley around the true data distribution.

---

## 4. Methodology & Constraints

### 4.1 Parameter Budgeting
To compare model architectures fairly, we solve for the exact Hidden Dimension ($H$) that results in a parameter count closest to the target `PARAMETER_BUDGET`.
*   **GAN/VAE**: The number of parameters in an MLP is quadratic with respect to layer width. We solve the quadratic equation $N_{params}(H) = P$ to find $H$.
*   **RBM**: The relation is linear ($N \approx D \cdot H$). We solve $H \approx P/D$.

### 4.2 Discretization (The "Binning" Feature)
The benchmarks support a `NUM_BINS` mode. When enabled ($>0$):
1.  **Preprocessing**: Continuous 1D data is digitized into $N$ bins and converted to One-Hot Vectors.
2.  **Model Change**:
    *   **RBM**: The Visible layer size becomes `NUM_BINS`. It learns the discrete correlations between bins.
    *   **GAN/VAE**: Input/Output dimensions expand to `NUM_BINS`.
3.  **Visualization**: The One-Hot outputs are "Soft Decoded" (or Hard Decoded) back to spatial coordinates to compute KLD against the ground truth.

### 4.3 Evaluation Metric (KLD)
We quantify performance using **Kullback-Leibler Divergence (KLD)** between the histogram of the true target distribution $P$ and the histogram of the model's generated samples $Q$.
$$ D_{KL}(P || Q) = \sum_i P_i \log \left( \frac{P_i}{Q_i + \epsilon} \right) $$
Lower KLD indicates better performance. An $\epsilon$ term is added for numerical stability.

---

## 5. Usage & Configuration

Configuration is located at the top of `generative_models_GAN_VAE_RBM_benchmarks.py`.

```python
TARGET_TYPE = "1D_TRIMODAL"   # or 1D_BIMODAL, 2D_MIX_GAUSS, 1D_POISSON...
PARAMETER_BUDGET = 1600       # Total learnable weights constraints
NUM_BINS = 20                 # Discretization bins (0 = Continuous)
SNAPSHOT_PERCENT = 5          # Snapshot interval
```

Run the benchmark:
```bash
python3 generative_models_GAN_VAE_RBM_benchmarks.py
```
