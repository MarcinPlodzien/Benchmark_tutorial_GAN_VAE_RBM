# Classical Generative Benchmarks

This repository implements a comparative framework for **Generative Adversarial Networks (GANs)**, **Variational Autoencoders (VAE)**, and **Restricted Boltzmann Machines (RBM)** using **JAX**.

The primary objective is to evaluate algorithmic efficiency under fixed constraints. By enforcing a **Universal Parameter Budget**, the benchmarks control for model capacity, facilitating direct comparison between diverse generative architectures.

---

## 1. Generative Adversarial Network (GAN)

### 1.1 Theoretical Framework
The GAN treats generative modeling as a zero-sum minimax game between two neural networks:
1.  **Generator ($G$)**: Attempts to map a latent noise vector $z \sim \mathcal{N}(0, I)$ to a sample $x_{fake}$ that is indistinguishable from real data.
2.  **Discriminator ($D$)**: Attempts to distinguish between real data samples $x_{real}$ and generated samples $x_{fake}$.

The Nash Equilibrium of this game occurs when the Generator recovers the true data distribution $p_{g} = p_{data}$. At this point, the optimal Discriminator $D^*(x)$ is:

$$ D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{g}(x)} = \frac{p_{data}(x)}{2p_{data}(x)} = 0.5 $$

Thus, a perfectly trained GAN is characterized by a Discriminator that outputs **0.5 (random guessing)** for all inputs, indicating it can no longer distinguish real data from generated data.

### 1.2 Architecture
*   **Generator**: A Multi-Layer Perceptron (MLP) mapping 16-dimensional latent noise to the data dimension $D_{out}$.
    *   *Input*: $z \in \mathbb{R}^{16}$
    *   *Hidden*: Dense Layer with `LeakyReLU` activation.
    *   *Output*: Linear Layer (Data Space).
*   **Discriminator**: An MLP mapping data to a scalar probability logit.
    *   *Input*: $x \in \mathbb{R}^{D_{in}}$
    *   *Hidden*: Dense Layer with `LeakyReLU` activation.
    *   *Output*: Scalar Linear (Logit).

### 1.3 Loss Function & Training Logic
The standard Minimax loss ($\min \log(1-D(G(z)))$) suffers from vanishing gradients early in training. We implement the **Non-Saturating Heuristic Loss**:

**1. Discriminator Step**:
Maximize the log-likelihood of correctly classifying real and fake data.

$$ \mathcal{L}_D = - \left( \mathbb{E}_{x \sim data}[\log D(x)] + \mathbb{E}_{z \sim noise}[\log(1 - D(G(z)))] \right) $$

*   **Implementation Detail**: We use **Label Smoothing** for the real class. Instead of targeting $1.0$, the Discriminator is trained to predict $0.9$. This regularizes the Discriminator, preventing it from becoming too confident and providing "flat" gradients to the Generator.

**2. Generator Step**:
Maximize the probability of the Discriminator being mistaken (classifying fake data as real).

$$ \mathcal{L}_G = - \mathbb{E}_{z \sim noise}[\log D(G(z))] $$

---

## 2. Variational Autoencoder (VAE)

### 2.1 Theoretical Framework
The VAE represents a Probabilistic Graphical Model. It assumes data is generated by unseen latent variables $z$. Since the true posterior $p(z|x)$ is intractable, the VAE uses **Variational Inference** to approximate it with a parametric distribution $q_\phi(z|x)$ (the Inference Network or Encoder).

### 2.2 Architecture
*   **Encoder (Inference Net)**: Maps input $x$ to the mean $\mu$ and log-variance $\log\sigma^2$ of the latent Gaussian.
    *   *Input*: $x \in \mathbb{R}^{D_{in}}$
    *   *Output*: $2 \times Z_{dim}$ (Mean vector + LogVar vector).
*   **Decoder (Generative Net)**: Maps sampled latent $z$ back to data space.
    *   *Input*: $z \in \mathbb{R}^{Z_{dim}}$
    *   *Output*: Reconstruction $\hat{x} \in \mathbb{R}^{D_{out}}$.

### 2.3 The Reparameterization Trick
To allow gradient descent to propagate through the stochastic sampling step $z \sim \mathcal{N}(\mu, \sigma^2)$, we reparameterize the randomness:

$$ z = \mu + \sigma \odot \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) $$

Now, the stochasticity is injected via $\epsilon$ (a constant input node), allowing $\mu$ and $\sigma$ to be differentiable parameters.

### 2.4 Loss Function (ELBO)
We minimize the Negative Evidence Lower Bound (ELBO):

$$ \mathcal{L}_{VAE} = \underbrace{ ||x - \hat{x}||^2 }_{\text{Reconstruction (MSE)}} + \beta \cdot \underbrace{ D_{KL}( \mathcal{N}(\mu, \sigma) || \mathcal{N}(0, I) ) }_{\text{Regularization}} $$

*   **Reconstruction**: Measures how well the VAE preserves information.
*   **KL Divergence**: Forces the latent space to approximate a standard Normal distribution, preventing overfitting and ensuring the latent space is continuous (smooth interpolation).
*   **$\beta$-VAE**: We use $\beta=0.1$ to weigh the reconstruction error higher than the KL term, which results in sharper samples at the cost of a slightly less regularized latent manifold.

### 2.5 Intuition: The Tug-of-War
Training a VAE is a balancing act between two opposing forces:
1.  **Reconstruction Loss (The "Memorizer")**: Wants to map each $x$ to a specific, unique $z$ point to minimize error. If unchecked, this leads to a "Dirac delta" latent space where $z$ carries no semantic structure (overfitting).
2.  **KL Divergence (The "Regularizer")**: Wants to smear $q(z|x)$ out to match the standard Normal prior $\mathcal{N}(0, I)$. This forces the latent codes to overlap and fill the space continuously.

**Convergence**: Ideally, the model finds a manifold where similar data points map to nearby latent codes (smoothness), allowing for meaningful interpolation, while distinct data points remain separable enough to be reconstructed. The "blurriness" often seen in VAEs typically results from the Gaussian assumption on $p(x|z)$ averaging out fine details to satisfy the regularization constraint.

---

## 3. Restricted Boltzmann Machine (RBM)

### 3.1 Theoretical Framework
The RBM is an **Energy-Based Model (EBM)**. It defines a scalar "Energy" $E(v, h)$ for every configuration of visible units $v$ (data) and hidden units $h$ (features). The probability of a state is given by the Boltzmann distribution:

$$ P(v) = \frac{\sum_h e^{-E(v,h)}}{Z} $$

where $Z$ is the partition function (normalization constant). Learning consists of shaping the energy landscape such that real data points have low energy (high probability) and noise has high energy.

### 3.2 Architecture
The RBM is a bipartite graph (no intra-layer connections):
*   **Visible Layer ($v$)**: Corresponds to the input data dimensions.
*   **Hidden Layer ($h$)**: Corresponds to latent feature detectors.
*   **Weights ($W$)**: A learnable matrix connecting every $v_i$ to every $h_j$.

### 3.3 Training Logic: Contrastive Divergence (CD-k)
Exact Maximum Likelihood training is intractable because computing $Z$ requires summing over all possible states. We approximate the gradient using **Contrastive Divergence**:

$$ \nabla \mathcal{L} \approx \underbrace{\nabla \text{FreeEnergy}(v_{data})}_{\text{Positive Phase}} - \underbrace{\nabla \text{FreeEnergy}(v_{model})}_{\text{Negative Phase}} $$

1.  **Positive Phase**: We clamp the visible units to the real data $v_{data}$ and compute the Free Energy. This pushes the energy of real data *down*.
2.  **Negative Phase**: We estimate the model's current distribution by running a **Gibbs Sampling** chain starting from the data.
    *   $P(h|v) = \sigma(vW + b_h)$
    *   $P(v|h) \sim \mathcal{N}(hW^T + b_v, \sigma^2)$
    *   After $k$ steps, we obtain $v_{model}$. We push the energy of this "fantasy particle" *up*.
    
This "push-down, push-up" mechanism creates an energy valley around the true data distribution.

### 3.4 Intuition: Sculpting the Energy Landscape
Imagine the Energy function $E(v)$ as a physical terrain.
*   **Likelihood** corresponds to **Low Energy**.
*   **Training** is the process of "digging holes" at the locations of real data points (Positive Phase) and "piling up dirt" at the locations where the model currently hallucinates/drifts (Negative Phase).
*   **Convergence**: Eventually, the "holes" become deep valleys at the real data locations, and the rest of the landscape is raised high. A particle dropped into this landscape (via Gibbs Sampling) will naturally roll down into the valleys, effectively generating data that looks real.

---

## 4. Target Distribution Zoology

We test the generative capabilities against a diverse set of synthetic distributions, ranging from simple Gaussian mixtures to discrete Poisson distributions.

### 4.1 Gaussian Mixtures (Multimodal)

The target probability density $P(x)$ is defined as an equally weighted mixture of $M$ Gaussian modes:

$$ P(x) = \frac{1}{M} \sum_{i=1}^M \mathcal{N}(x | \mu_i, \sigma^2 I) $$

*   **`1D_BIMODAL`**: Two modes at $\mu \in \{-1.5, 1.5\}$ with $\sigma=0.4$. Tests ability to capture separated clusters.
*   **`1D_TRIMODAL`**: Three modes at $\mu \in \{-2.0, 0.0, 2.0\}$ with $\sigma=0.35$.
*   **`1D_QUADMODAL`**: Four modes at $\mu \in \{-2.5, -0.8, 0.8, 2.5\}$ with $\sigma=0.3$. Tests high-frequency mode collapse resilience.
*   **`2D_MIX_GAUSS`**: Four modes at the corners of a square $\mu \in \{ (\pm 1.5, \pm 1.5) \}$ with $\sigma=0.5$.

### 4.2 Skewed Distribution (Beta)

*   **`1D_BETA`**: A Beta distribution mapped to the domain $[-2.5, 2.5]$.

$$ x \sim \text{Beta}(\alpha=2, \beta=5) $$

This creates a right-skewed distribution, testing the model's ability to learn asymmetry.

### 4.3 Discrete Distribution (Poisson)

*   **`1D_POISSON`**: A discrete Poisson distribution shifted to be centered near zero.

$$ x \sim \text{Poisson}(\lambda=4) - \lambda $$

Use with `NUM_BINS > 0` to treat this as a true discrete generation task.

---

## 5. Methodology & Constraints

### 5.1 Parameter Budgeting
To compare model architectures fairly, we solve for the exact Hidden Dimension ($H$) that results in a parameter count closest to the target `PARAMETER_BUDGET`.
*   **GAN/VAE**: The number of parameters in an MLP is quadratic with respect to layer width. We solve the quadratic equation $N_{params}(H) = P$ to find $H$.
*   **RBM**: The relation is linear ($N \approx D \cdot H$). We solve $H \approx P/D$.

### 5.2 Discretization (The "Binning" Feature)
The benchmarks support a `NUM_BINS` mode. When enabled ($>0$):
1.  **Preprocessing**: Continuous 1D data is digitized into $N$ bins and converted to One-Hot Vectors.
2.  **Model Change**:
    *   **RBM**: The Visible layer size becomes `NUM_BINS`. It learns the discrete correlations between bins.
    *   **GAN/VAE**: Input/Output dimensions expand to `NUM_BINS`.
3.  **Visualization**: The One-Hot outputs are "Soft Decoded" (or Hard Decoded) back to spatial coordinates to compute KLD against the ground truth.

### 5.3 Metric 1: Kullback-Leibler Divergence (KLD)
We quantify performance using **Kullback-Leibler Divergence (KLD)** between the histogram of the true target distribution $P$ and the histogram of the model's generated samples $Q$.

$$
D_{KL}(P || Q) = \sum_i P_i \log \left( \frac{P_i}{Q_i + \epsilon} \right)
$$

Lower KLD indicates better performance. An $\epsilon$ term is added for numerical stability.

### 5.4 Metric 2: Wasserstein Distance (Earth Mover's Distance)

While KLD measures "overlap" (information theoretic), the **Wasserstein Distance** measures "geometry" (transport cost), making it effective for disjoint distributions where KLD fails.

#### 5.4.1 Conceptual Framework: Optimal Transport
To understand the distinct advantage of Wasserstein Distance, we view probability distributions through the lens of Optimal Transport. Consider two distributions:
1.  **Distribution P** (The Model): A mass distribution $P(x)$.
2.  **Distribution Q** (The Target): A capacity distribution $Q(x)$.

The Wasserstein Distance is defined as the **minimum cost** required to transport the mass from state $P$ to state $Q$.

$$
\text{Cost} = \text{Mass} \times \text{Distance Moved}
$$

**Comparative Analysis: Disjoint Support**
Consider a scenario where the model $P$ is concentrated at $x=0$ and the target $Q$ is concentrated at $x=10$.

*   **KLD (Overlap-based)**:
    Since $P(x) \cdot Q(x) = 0$ everywhere, the divergence is maximized (or infinite). Crucially, the gradient is zero because small local perturbations of $P$ (e.g., moving to $x=0.1$) do not create overlap. The optimizer has no signal.

*   **Wasserstein (Geometry-based)**:
    The metric calculates the physical work required to move the mass from $0$ to $10$.

    $$
    W_1 = 10 \times 1.0 = 10
    $$

    If the model updates and moves its mass to $1$, the cost reduces to $9$. This provides a smooth, non-zero gradient ($ \nabla \mathcal{L} \neq 0 $) that guides the model towards the target.

#### 5.4.2 Mathematical Formulation (The 1D Case)
In one dimension, the optimal transport plan has a closed-form solution involving the **Cumulative Distribution Functions (CDF)**.

**Definition**:
The CDF $F(x)$ aggregates the probability mass accumulated up to point $x$:

$$
F_P(x) = \int_{-\infty}^{x} P(t) \, dt \quad (\text{or } \sum_{t \le x} P(t))
$$

**Integral Formula**:
The Wasserstein-1 distance is exactly the $L_1$ distance between the two CDFs:

$$
W_1(P, Q) = \int_{-\infty}^{\infty} | F_P(x) - F_Q(x) | dx
$$

#### 5.4.3 Computational Intuition (Quantiles)
While the integral definition is theoretically robust, in practice (for finite samples), the Wasserstein-1 distance has an equivalent, computationally tangible interpretation:

**The Mean Absolute Difference of Quantiles**
If we sort both the model's samples $X_{model}$ and the target samples $X_{target}$ in ascending order, the Wasserstein distance is simply the average distance between the $k$-th smallest sorted sample of each set.

$$
W_1 \approx \frac{1}{N} \sum_{k=1}^{N} | \text{sorted}(X_{model})_k - \text{sorted}(X_{target})_k |
$$

This interpretation connects the geometry of the distribution directly to the numerical implementation (e.g., `scipy.stats.wasserstein_distance`).

---

## 6. Advanced Features

### 6.1 Gumbel-Softmax (Discrete GAN)
When `NUM_BINS > 0`, the GAN Generator uses the **Gumbel-Softmax** trick to output differentiable discrete approximations.

$$
y = \text{Softmax}\left( \frac{\log(\pi) + g}{\tau} \right)
$$

This allows the GAN to learn categorical distributions effectively, competing fairly with the discrete RBM.

### 6.2 VAE Latent Manifold Visualization
To verify that the VAE has learned a meaningful continuous representation rather than simply memorizing training data, we employ a **Latent Manifold Sweep**.

#### Conceptual Objective
A key property of VAEs is the **continuity** of the latent space. Unlike standard autoencoders, a VAE forces the latent code $z$ to follow a Gaussian prior $\mathcal{N}(0, I)$. This implies that:
1.  **Interpolation**: Moving smoothly from $z_A$ to $z_B$ should result in a smooth semantic transformation in the data space $x$.
2.  **Completeness**: Any point sampled from the prior should map to a valid data point.

#### Determining Smoothness
We iterate the latent variables $z_1, z_2$ across a 2D grid from $[-3, 3]$ and visualize the decoder's expected output $E[x|z]$.
*   **Success**: The visualization shows smooth transitions between the modes (e.g., gradually deforming one spike into another).
*   **Failure**: The visualization shows sharp, disjoint boundaries or "holes" (regions of noise), indicating a failure to regularize the manifold.

**Output File**: `vae_latent_manifold_sweep.png`
(Look for this file in `results_classical_benchmarks/` after running the script).

---

## 7. Usage & Configuration

Configuration is located at the top of `generative_models_GAN_VAE_RBM_benchmarks.py`.

```python
TARGET_TYPE = "1D_TRIMODAL"   # or 1D_BIMODAL, 2D_MIX_GAUSS, 1D_POISSON...
PARAMETER_BUDGET = 1600       # Total learnable weights constraints
NUM_BINS = 20                 # Discretization bins (0 = Continuous)
SNAPSHOT_PERCENT = 5          # Snapshot interval
```

Run the benchmark:
```bash
python3 generative_models_GAN_VAE_RBM_benchmarks.py
```

## 8. Key Outputs
All results are saved to `results_classical_benchmarks/`.

| Filename | Description |
| :--- | :--- |
| `metrics_comparison.png` | **Primary Dashboard**. Shows KLD (Left) and Wasserstein Distance (Right) vs Epochs. |
| `classical_benchmark_comparison_hist.png` | **Final Histogram**. Side-by-side comparison of Target density vs generated samples. |
| `vae_latent_manifold_sweep.png` | **VAE Manifold**. (Section 6.2) Visualizes the decoder output $E[x|z]$ as we sweep latent $z$. |
| `timeline_*.png` | **Training Evolution**. Snapshots of the distributions at 5%, 10%, ... 100% of training. |
